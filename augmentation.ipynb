{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ea701e-22d6-4f4c-86ef-3cd28010eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "##augmentation bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab7149b5-e75b-4138-9a78-1662d09cda72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "data = pd.read_csv('data/public_data_dev/track_a/train/eng.csv')\n",
    "# device = torch.device('cpu')\n",
    "# print(torch.device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8247f9b9-9457-43d6-ae7c-2f91fd061491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-label backtranslation only\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load translation models\n",
    "en_to_fr_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\").to(device)\n",
    "en_to_fr_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "\n",
    "fr_to_en_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\").to(device)\n",
    "fr_to_en_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "\n",
    "# Back-translation function\n",
    "def back_translate(text, en_to_x_model, en_to_x_tokenizer, x_to_en_model, x_to_en_tokenizer, device):\n",
    "    try:\n",
    "        # Translate from English to target language\n",
    "        en_to_x = en_to_x_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        x_output = en_to_x_model.generate(**en_to_x)\n",
    "        x_text = en_to_x_tokenizer.decode(x_output[0], skip_special_tokens=True)\n",
    "\n",
    "        # Translate back from target language to English\n",
    "        x_to_en = x_to_en_tokenizer(x_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        en_output = x_to_en_model.generate(**x_to_en)\n",
    "        back_translated_text = x_to_en_tokenizer.decode(en_output[0], skip_special_tokens=True)\n",
    "\n",
    "        return back_translated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during back-translation: {e}\")\n",
    "        return text  # Return original text if an error occurs\n",
    "\n",
    "# Augmentation function for multi-label data\n",
    "def augment_minority_classes(data, classes, target_size, device):\n",
    "    # Find samples that belong to any minority class\n",
    "    minority_samples = data[data[classes].sum(axis=1) > 0]\n",
    "    augmented_texts = []\n",
    "\n",
    "    # Calculate the number of samples needed for each class\n",
    "    class_sizes = data[classes].sum()\n",
    "    samples_needed = {cls: max(0, target_size - class_sizes[cls]) for cls in classes}\n",
    "\n",
    "    # Progress bar for augmentation\n",
    "    for cls in classes:\n",
    "        if samples_needed[cls] <= 0:\n",
    "            continue  # Skip if no augmentation is needed\n",
    "\n",
    "        class_texts = minority_samples[minority_samples[cls] == 1]\n",
    "        for i in tqdm(range(samples_needed[cls]), desc=f\"Augmenting {cls}\", unit=\"sample\"):\n",
    "            text = class_texts.iloc[i % len(class_texts)]['text']\n",
    "            labels = class_texts.iloc[i % len(class_texts)][classes].values\n",
    "\n",
    "            # Back-translate the text\n",
    "            augmented_text = back_translate(\n",
    "                text, en_to_fr_model, en_to_fr_tokenizer, fr_to_en_model, fr_to_en_tokenizer, device\n",
    "            )\n",
    "\n",
    "            # Append augmented text with original labels\n",
    "            augmented_texts.append({'text': augmented_text, **dict(zip(classes, labels))})\n",
    "\n",
    "    return pd.DataFrame(augmented_texts)\n",
    "\n",
    "# Loading  dataset\n",
    "data = pd.read_csv(\"data/public_data_dev/track_a/train/eng.csv\") \n",
    "\n",
    "# classes to balance\n",
    "classes = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n",
    "\n",
    "# Handle missing values\n",
    "data[classes] = data[classes].fillna(0)\n",
    "\n",
    "# Calculate target size (size of the largest class)\n",
    "largest_class_size = data[classes].sum().max()\n",
    "\n",
    "# Augment minority classes\n",
    "augmented_data = augment_minority_classes(data, classes, largest_class_size, device)\n",
    "\n",
    "# Merge original and augmented data\n",
    "balanced_data = pd.concat([data, augmented_data], ignore_index=True)\n",
    "\n",
    "# Handle missing values in augmented dataset\n",
    "balanced_data[classes] = balanced_data[classes].fillna(0)\n",
    "\n",
    "# Print original and balanced dataset sizes\n",
    "print(f\"Original dataset size: {len(data)}\")\n",
    "print(f\"Balanced dataset size: {len(balanced_data)}\")\n",
    "\n",
    "# Save the balanced dataset to a CSV file\n",
    "output_file_path = '/home/h5/hama461f/Project/backtranslated.csv'\n",
    "balanced_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Confirm the path where the file saved\n",
    "print(f\"Balanced dataset saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee631367-21c0-4757-8e60-d5f7e35f6827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Augmenting anger (x3): 100%|██████████| 666/666 [05:24<00:00,  2.05sample/s]\n",
      "Augmenting joy (x2): 100%|██████████| 674/674 [05:19<00:00,  2.11sample/s]\n",
      "Augmenting sadness (x2): 100%|██████████| 878/878 [06:44<00:00,  2.17sample/s]\n",
      "Augmenting surprise (x2): 100%|██████████| 839/839 [05:45<00:00,  2.43sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 2768\n",
      "Balanced dataset size: 5825\n",
      "Balanced dataset saved to /home/h5/hama461f/Project/2attbalanced_data_paraphrase_fixed.csv\n"
     ]
    }
   ],
   "source": [
    "#multi label paraphrapse and back translate\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import MarianMTModel, MarianTokenizer, PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load translation models for multiple language pairs and store them in a dictionary\n",
    "def load_translation_models(lang_pair, device):\n",
    "    en_to_x_model = MarianMTModel.from_pretrained(f\"Helsinki-NLP/opus-mt-{lang_pair[0]}-{lang_pair[1]}\").to(device)\n",
    "    en_to_x_tokenizer = MarianTokenizer.from_pretrained(f\"Helsinki-NLP/opus-mt-{lang_pair[0]}-{lang_pair[1]}\")\n",
    "    x_to_en_model = MarianMTModel.from_pretrained(f\"Helsinki-NLP/opus-mt-{lang_pair[1]}-{lang_pair[0]}\").to(device)\n",
    "    x_to_en_tokenizer = MarianTokenizer.from_pretrained(f\"Helsinki-NLP/opus-mt-{lang_pair[1]}-{lang_pair[0]}\")\n",
    "    return en_to_x_model, en_to_x_tokenizer, x_to_en_model, x_to_en_tokenizer\n",
    "\n",
    "# Preload all translation models for different language pairs\n",
    "language_pairs = [\n",
    "    ('en', 'fr'),  # English to French\n",
    "    ('en', 'de'),  # English to German\n",
    "    ('en', 'es'),  # English to Spanish\n",
    "    ('en', 'it')   # English to Italian\n",
    "]\n",
    "\n",
    "translation_models = {}\n",
    "\n",
    "# Load models for each language pair and store them in a dictionary\n",
    "for lang_pair in language_pairs:\n",
    "    en_to_x_model, en_to_x_tokenizer, x_to_en_model, x_to_en_tokenizer = load_translation_models(lang_pair, device)\n",
    "    translation_models[lang_pair] = {\n",
    "        'en_to_x_model': en_to_x_model,\n",
    "        'en_to_x_tokenizer': en_to_x_tokenizer,\n",
    "        'x_to_en_model': x_to_en_model,\n",
    "        'x_to_en_tokenizer': x_to_en_tokenizer\n",
    "    }\n",
    "\n",
    "# Load paraphrasing model\n",
    "paraphrase_model_name = \"tuner007/pegasus_paraphrase\"\n",
    "paraphrase_model = PegasusForConditionalGeneration.from_pretrained(paraphrase_model_name).to(device)\n",
    "paraphrase_tokenizer = PegasusTokenizer.from_pretrained(paraphrase_model_name)\n",
    "\n",
    "# Define back-translation function with error handling\n",
    "def back_translate(text, en_to_x_model, en_to_x_tokenizer, x_to_en_model, x_to_en_tokenizer, device):\n",
    "    try:\n",
    "        # Translate from English to target language\n",
    "        en_to_x = en_to_x_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        x_output = en_to_x_model.generate(**en_to_x)\n",
    "        x_text = en_to_x_tokenizer.decode(x_output[0], skip_special_tokens=True)\n",
    "\n",
    "        # Translate back from target language to English\n",
    "        x_to_en = x_to_en_tokenizer(x_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        en_output = x_to_en_model.generate(**x_to_en)\n",
    "        back_translated_text = x_to_en_tokenizer.decode(en_output[0], skip_special_tokens=True)\n",
    "\n",
    "        return back_translated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during back-translation: {e}\")\n",
    "        return text  # Return original text if an error occurs\n",
    "\n",
    "# Paraphrasing function using Pegasus with error handling\n",
    "def paraphrase_text_pegasus(text):\n",
    "    try:\n",
    "        inputs = paraphrase_tokenizer([text], max_length=60, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        paraphrase = paraphrase_model.generate(inputs[\"input_ids\"], max_length=60, num_beams=5, num_return_sequences=1, early_stopping=True).to(device)\n",
    "        paraphrased_text = paraphrase_tokenizer.decode(paraphrase[0], skip_special_tokens=True)\n",
    "        return paraphrased_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during paraphrasing: {e}\")\n",
    "        return text  # Return original text if an error occurs\n",
    "\n",
    "# Augment text using back-translation and paraphrasing\n",
    "def augment_text(text, language_pairs, device):\n",
    "    # Back-translate using a fixed language pair\n",
    "    lang_pair = random.choice(language_pairs)\n",
    "    models = translation_models[lang_pair]\n",
    "    augmented_text = back_translate(text, models['en_to_x_model'], models['en_to_x_tokenizer'], models['x_to_en_model'], models['x_to_en_tokenizer'], device)\n",
    "    \n",
    "    # Paraphrase the back-translated text\n",
    "    paraphrased_text = paraphrase_text_pegasus(augmented_text)\n",
    "    \n",
    "    return paraphrased_text\n",
    "\n",
    "# Augmentation function for minority classes with controlled augmentation factor\n",
    "def augment_minority_classes_with_variability(data, classes, augmentation_factors, device, language_pairs):\n",
    "    augmented_dfs = []\n",
    "\n",
    "    for cls, factor in augmentation_factors.items():\n",
    "        class_texts = data[data[cls] == 1]\n",
    "        original_size = len(class_texts)\n",
    "        samples_needed = int(original_size * factor) - original_size\n",
    "\n",
    "        if samples_needed <= 0:\n",
    "            continue  # Skip if no augmentation is needed\n",
    "\n",
    "        augmented_texts = []\n",
    "\n",
    "        for i in tqdm(range(samples_needed), desc=f\"Augmenting {cls} (x{factor})\", unit=\"sample\"):\n",
    "            text = class_texts.iloc[i % original_size]['text']\n",
    "            labels = class_texts.iloc[i % original_size][classes].values\n",
    "            \n",
    "            # Augment text\n",
    "            augmented_text = augment_text(text, language_pairs, device)\n",
    "            \n",
    "            # Append augmented text with original labels\n",
    "            augmented_texts.append({'text': augmented_text, **dict(zip(classes, labels))})\n",
    "\n",
    "        augmented_dfs.append(pd.DataFrame(augmented_texts))\n",
    "\n",
    "    return pd.concat(augmented_dfs, ignore_index=True)\n",
    "\n",
    "# dataset\n",
    "data = pd.read_csv(\"data/public_data_dev/track_a/train/eng.csv\") \n",
    "\n",
    "# Define augmentation factors for each class\n",
    "augmentation_factors = {\n",
    "    'anger': 3,  # Augment anger class 3x\n",
    "    'fear': 0,   \n",
    "    'joy': 2,    # Augment joy class 2x\n",
    "    'sadness': 2, # augment sadness class 2x\n",
    "    'surprise': 2 # Augment surprise class 2x\n",
    "}\n",
    "\n",
    "# Augment minority classes\n",
    "augmented_data = augment_minority_classes_with_variability(data, ['anger', 'fear', 'joy', 'sadness', 'surprise'], augmentation_factors, device, language_pairs)\n",
    "\n",
    "# Combine original and augmented data\n",
    "balanced_data = pd.concat([data, augmented_data], ignore_index=True)\n",
    "\n",
    "# Handle missing values in the augmented dataset\n",
    "balanced_data[['anger', 'fear', 'joy', 'sadness', 'surprise']] = balanced_data[['anger', 'fear', 'joy', 'sadness', 'surprise']].fillna(0)\n",
    "\n",
    "# Print original and balanced dataset sizes\n",
    "print(f\"Original dataset size: {len(data)}\")\n",
    "print(f\"Balanced dataset size: {len(balanced_data)}\")\n",
    "\n",
    "# Save the balanced dataset to a CSV file\n",
    "output_file_path = '/home/h5/hama461f/Project/backtrans_paraph_3222.csv'\n",
    "balanced_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Confirm the path where the file is saved\n",
    "print(f\"Balanced dataset saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbbab6bb-4900-4e80-8d4c-643073b2d8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Augmenting anger (x5): 100%|██████████| 1332/1332 [09:52<00:00,  2.25sample/s]\n",
      "Augmenting joy (x2): 100%|██████████| 674/674 [05:21<00:00,  2.10sample/s]\n",
      "Augmenting sadness (x2): 100%|██████████| 878/878 [06:47<00:00,  2.16sample/s]\n",
      "Augmenting surprise (x2): 100%|██████████| 839/839 [05:49<00:00,  2.40sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 2768\n",
      "Balanced dataset size: 6491\n",
      "\n",
      "Class distribution in balanced dataset:\n",
      "anger       1938\n",
      "fear        3938\n",
      "joy         1580\n",
      "sadness     2619\n",
      "surprise    2483\n",
      "dtype: int64\n",
      "Balanced dataset saved to /home/h5/hama461f/Project/3attequalbalanced_data_paraphrase_fixed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#2nd attempt multi label paraphrapse and back translate 5222\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import MarianMTModel, MarianTokenizer, PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load translation models for multiple language pairs and store them in a dictionary\n",
    "def load_translation_models(lang_pair, device):\n",
    "    en_to_x_model = MarianMTModel.from_pretrained(f\"Helsinki-NLP/opus-mt-{lang_pair[0]}-{lang_pair[1]}\").to(device)\n",
    "    en_to_x_tokenizer = MarianTokenizer.from_pretrained(f\"Helsinki-NLP/opus-mt-{lang_pair[0]}-{lang_pair[1]}\")\n",
    "    x_to_en_model = MarianMTModel.from_pretrained(f\"Helsinki-NLP/opus-mt-{lang_pair[1]}-{lang_pair[0]}\").to(device)\n",
    "    x_to_en_tokenizer = MarianTokenizer.from_pretrained(f\"Helsinki-NLP/opus-mt-{lang_pair[1]}-{lang_pair[0]}\")\n",
    "    return en_to_x_model, en_to_x_tokenizer, x_to_en_model, x_to_en_tokenizer\n",
    "\n",
    "# Preload all translation models for different language pairs\n",
    "language_pairs = [\n",
    "    ('en', 'fr'),  # English to French\n",
    "    ('en', 'de'),  # English to German\n",
    "    ('en', 'es'),  # English to Spanish\n",
    "    ('en', 'it')   # English to Italian\n",
    "]\n",
    "\n",
    "translation_models = {}\n",
    "\n",
    "# Load models for each language pair and store them in a dictionary\n",
    "for lang_pair in language_pairs:\n",
    "    en_to_x_model, en_to_x_tokenizer, x_to_en_model, x_to_en_tokenizer = load_translation_models(lang_pair, device)\n",
    "    translation_models[lang_pair] = {\n",
    "        'en_to_x_model': en_to_x_model,\n",
    "        'en_to_x_tokenizer': en_to_x_tokenizer,\n",
    "        'x_to_en_model': x_to_en_model,\n",
    "        'x_to_en_tokenizer': x_to_en_tokenizer\n",
    "    }\n",
    "\n",
    "# Load paraphrasing model\n",
    "paraphrase_model_name = \"tuner007/pegasus_paraphrase\"\n",
    "paraphrase_model = PegasusForConditionalGeneration.from_pretrained(paraphrase_model_name).to(device)\n",
    "paraphrase_tokenizer = PegasusTokenizer.from_pretrained(paraphrase_model_name)\n",
    "\n",
    "# Define back-translation function with error handling\n",
    "def back_translate(text, en_to_x_model, en_to_x_tokenizer, x_to_en_model, x_to_en_tokenizer, device):\n",
    "    try:\n",
    "        # Translate from English to target language\n",
    "        en_to_x = en_to_x_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        x_output = en_to_x_model.generate(**en_to_x)\n",
    "        x_text = en_to_x_tokenizer.decode(x_output[0], skip_special_tokens=True)\n",
    "\n",
    "        # Translate back from target language to English\n",
    "        x_to_en = x_to_en_tokenizer(x_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        en_output = x_to_en_model.generate(**x_to_en)\n",
    "        back_translated_text = x_to_en_tokenizer.decode(en_output[0], skip_special_tokens=True)\n",
    "\n",
    "        return back_translated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during back-translation: {e}\")\n",
    "        return text  # Return original text if an error occurs\n",
    "\n",
    "# Paraphrasing function using Pegasus with error handling\n",
    "def paraphrase_text_pegasus(text):\n",
    "    try:\n",
    "        inputs = paraphrase_tokenizer([text], max_length=60, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        paraphrase = paraphrase_model.generate(inputs[\"input_ids\"], max_length=60, num_beams=5, num_return_sequences=1, early_stopping=True).to(device)\n",
    "        paraphrased_text = paraphrase_tokenizer.decode(paraphrase[0], skip_special_tokens=True)\n",
    "        return paraphrased_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during paraphrasing: {e}\")\n",
    "        return text  # Return original text if an error occurs\n",
    "\n",
    "# Augment text using back-translation and paraphrasing\n",
    "def augment_text(text, language_pairs, device):\n",
    "    # Back-translate using a fixed language pair\n",
    "    lang_pair = random.choice(language_pairs)\n",
    "    models = translation_models[lang_pair]\n",
    "    augmented_text = back_translate(text, models['en_to_x_model'], models['en_to_x_tokenizer'], models['x_to_en_model'], models['x_to_en_tokenizer'], device)\n",
    "    \n",
    "    # Paraphrase the back-translated text\n",
    "    paraphrased_text = paraphrase_text_pegasus(augmented_text)\n",
    "    \n",
    "    return paraphrased_text\n",
    "\n",
    "# Augmentation function for minority classes with controlled augmentation factor\n",
    "def augment_minority_classes_with_variability(data, classes, augmentation_factors, device, language_pairs):\n",
    "    augmented_dfs = []\n",
    "\n",
    "    for cls, factor in augmentation_factors.items():\n",
    "        class_texts = data[data[cls] == 1]\n",
    "        original_size = len(class_texts)\n",
    "        samples_needed = int(original_size * factor) - original_size\n",
    "\n",
    "        if samples_needed <= 0:\n",
    "            continue  # Skip if no augmentation is needed\n",
    "\n",
    "        augmented_texts = []\n",
    "\n",
    "        for i in tqdm(range(samples_needed), desc=f\"Augmenting {cls} (x{factor})\", unit=\"sample\"):\n",
    "            text = class_texts.iloc[i % original_size]['text']\n",
    "            labels = class_texts.iloc[i % original_size][classes].values\n",
    "            \n",
    "            # Augment text\n",
    "            augmented_text = augment_text(text, language_pairs, device)\n",
    "            \n",
    "            # Append augmented text with original labels\n",
    "            augmented_texts.append({'text': augmented_text, **dict(zip(classes, labels))})\n",
    "\n",
    "        augmented_dfs.append(pd.DataFrame(augmented_texts))\n",
    "\n",
    "    return pd.concat(augmented_dfs, ignore_index=True)\n",
    "\n",
    "# dataset\n",
    "data = pd.read_csv(\"data/public_data_dev/track_a/train/eng.csv\")  # Replace with your dataset path\n",
    "\n",
    "# Define augmentation factors for each class\n",
    "augmentation_factors = {\n",
    "    'anger': 5,  # Augment anger class 5x\n",
    "    'fear': 0,   \n",
    "    'joy': 2,    # Augment joy class 2x\n",
    "    'sadness': 2, # Augment sadness class 2x\n",
    "    'surprise': 2 # Augment surprise class 2x\n",
    "}\n",
    "\n",
    "# Augment minority classes\n",
    "augmented_data = augment_minority_classes_with_variability(data, ['anger', 'fear', 'joy', 'sadness', 'surprise'], augmentation_factors, device, language_pairs)\n",
    "\n",
    "# Combine original and augmented data\n",
    "balanced_data = pd.concat([data, augmented_data], ignore_index=True)\n",
    "\n",
    "# Handle missing values in the augmented dataset\n",
    "balanced_data[['anger', 'fear', 'joy', 'sadness', 'surprise']] = balanced_data[['anger', 'fear', 'joy', 'sadness', 'surprise']].fillna(0)\n",
    "\n",
    "# Print original and balanced dataset sizes\n",
    "print(f\"Original dataset size: {len(data)}\")\n",
    "print(f\"Balanced dataset size: {len(balanced_data)}\")\n",
    "\n",
    "# Print class distribution in balanced dataset\n",
    "class_counts = balanced_data[['anger', 'fear', 'joy', 'sadness', 'surprise']].sum()\n",
    "print(\"\\nClass distribution in balanced dataset:\")\n",
    "print(class_counts)\n",
    "\n",
    "# Save the balanced dataset to a CSV file\n",
    "output_file_path = '/home/h5/hama461f/Project/backtrans_paraph_3222.csv'\n",
    "balanced_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Confirm the path where the file is saved\n",
    "print(f\"Balanced dataset saved to {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
